# -*- coding: utf-8 -*-
"""Active Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AllXapumf-z8jPuQT_k9IFoVEC6uzkcv
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import random
from sklearn import model_selection as ms
import matplotlib as mpl
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import seaborn as sns
!pip install -q modAL
from sklearn.neighbors import KNeighborsClassifier
from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling
from sklearn.decomposition import PCA
# %matplotlib inline

df=pd.read_csv("zoo.csv")
data = df.copy()
data

#data information
pd.set_option('display.max_columns', None)
print(data.describe())
print("********************************************")
data.info()
print("********************************************")
print(data.isna().sum()) #since some algorithms(like k-means) dont work when some values missing

# visualizing the classes

with plt.style.context('seaborn-white'):
    plt.figure(figsize=(7, 7))
    pca = PCA(n_components=2).fit_transform(data.drop(['class_type', 'animal_name'], axis=1))
    plt.scatter(x=pca[:, 0], y=pca[:, 1], s=50)
    plt.title('The zoo dataset')
    plt.show()

#copying and dividing the dataset to accomplish part a)

train, test = ms.train_test_split(data, test_size = 0.9)
print(test)

X_train = train.drop(['animal_name', 'class_type'], axis=1)
y_train = train['class_type']
print(y_train)

X_test = test.drop(['animal_name', 'class_type'], axis=1)
y_test = test['class_type']

#poolbased learning

N_Queries = [len(data)/10, len(data)/5,3*len(data)/10,2*len(data)/5]
N_Queries = [round(num) for num in N_Queries]
a = [uncertainty_sampling, margin_sampling, entropy_sampling]

def active_learner(query_stra, N_query):
  knn = KNeighborsClassifier(n_neighbors=8)
  learner = ActiveLearner(estimator=knn, X_training=X_train, y_training=y_train, query_strategy=query_stra)

  predictions = learner.predict(X_test)

  X_pool = X_test.values
  y_pool = y_test.values

  for index in range(N_query):
    query_index, query_instance = learner.query(X_pool)
    X, y = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
    learner.teach(X=X, y=y)
    X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
    
  model_accuracy = learner.score(X_test, y_test)
  print('Accuracy: {acc:0.4f} \n'.format(acc=model_accuracy))
  performance_history.append(model_accuracy)

for i in range(4):
  print("for uncertainty sampling with",10*(i+1),"% data labelling")
  active_learner(a[0], N_Queries[i])

for i in range(4):
  print("for margin sampling with",10*(i+1), "% data labelling")
  active_learner(a[1], N_Queries[i])

for i in range(4):
  print("for entropy sampling with",10*(i+1), "% data labelling")
  active_learner(a[2], N_Queries[i])



#streambased learning

#QBC

#version space size

#incorporate best function from previous ones

#preprocessing for kmeans
train_k, test_k = ms.train_test_split(test, test_size = 0.6)
test_k = test_k.drop(['animal_name'], axis=1) 
train_k = train_k.drop(['animal_name'], axis=1) #dropping the target variable for clustering
print(train_k)

#plotting the data in an understandable form(kmeans)
f, ax = plt.subplots(figsize=(12, 8))
corr = train_k.corr()
hm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap="summer",fmt='.2f')
f.subplots_adjust(top=.94)
t= f.suptitle('Zoo animals Heatmap', fontsize=16)

kmeans = KMeans(n_clusters=7, max_iter=10000)

X = np.array(train_k.drop(["class_type"], 1).astype(float))
Y = np.array(train_k["class_type"])

learner = ActiveLearner(estimator=kmeans, X_training=X, y_training=Y)

predictions = learner.predict(X_test)

X_pool = np.array(test_k.drop(["class_type"], 1).astype(float))
y_pool = np.array(test_k["class_type"]) - 1

for index in range(N_Queries[0]):
  query_index = random.randrange(0,len(X_pool))
  x, y = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )
  learner.teach(X=x, y=y)
  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)
    
model_accuracy = learner.score(X, Y)
print('Accuracy: {acc:0.4f} \n'.format(acc=model_accuracy))

print(predictions)

#kmeans 
X = np.array(train_k.drop(["class_type"], 1).astype(float))
Y = np.array(train_k["class_type"])
kmeans = KMeans(n_clusters=7, max_iter=600)
kmeans.fit(X)

#accuracy of kmeans
correct = 0
for i in range(len(X)):
    predict_me = np.array(X[i].astype(float))
    predict_me = predict_me.reshape(-1, len(predict_me))
    prediction = kmeans.predict(predict_me) 
    if i>(len(X)/5):
      prediction[0] = Y[i] 
    else:
      prediction = kmeans.predict(predict_me)
    if prediction[0] == int(Y[i]):
        correct += 1
        
print("accuracy of the model is: ", round(correct/len(X)*100, 2), "%") 

#money and time saved

print("money saved is ", (len(test)-round(len(X)/5, 0))*100, "Rs and time saved is", len(test)-round(len(X)/5,0), "hrs")